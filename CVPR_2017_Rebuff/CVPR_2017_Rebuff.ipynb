{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iCarRL: Incremental Classifier and Representation Learning.\n",
    "\n",
    "__Sylvestre-Alvise Rebuffi- University of Oxford/IST Austria__\n",
    "\n",
    "__CVPR_2017__\n",
    "\n",
    "This paper proposed a method that learns data representation and Incremental Learning Classifier (using 32-Layer ResNet) simutaneously. The paper experiments on image datasets (CIFAR100, ILSVRC).\n",
    ". \n",
    "<img src=\"Figure_1.jpg\" width=\"400\">\n",
    "\n",
    "**Training/Testing Procudure**. The image dataset is split into training/testing sets with a ratio (let's say 90/10). The training is conducted online, where a portion of training sets is fed into the network in each training step. For example, at  1st training step, we can feed 2 classes of dog an cat (600 images/class); at 2nd training step, images of next 2 classes are fed into the network. We can also change the number of classes in each training step to 5,10,...etc. The trained networks in each training steps are tested on testing datasets (10% of data). However, we only test on images of the trained classes. \n",
    "\n",
    "**Data Representation**. For every incremental step, the samples (images) of each class for old and new classes are constructed. The total samples must not be excceed K, thus each class will have max $m =K/t$, given t classes are presented.\n",
    "<img src=\"Figure_2.jpg\" width=\"400\">\n",
    "\n",
    "**Incremental Learning** \n",
    "Given m samples of each class (old and new ones), the steps of incremental learning is as follows:\n",
    "1. All samples are gathered into set $D$\n",
    "2. Calculate network outputs with pre-update network parameters. These outputs are used to minimize\n",
    "the distillation loss in next steps.\n",
    "3. All samples in set $D$ are used to minimize the loss function consisting of **distillation loss** and **classification loss**. The intuition of distillation loss is that we want the new (updated) network weights to generate the same outputs for samples of old classes in step 2. This helps solve the catastrophic forgeting problem. At the same time, the classification loss discriminates samples in entire classes. \n",
    "The algorithm is as followed. \n",
    "<img src=\"Figure_3.jpg\" width=\"400\">\n",
    "\n",
    "**Classification**: The idea is simmilar to the concept of K-Means. The mean of exemplars (samples that are selected) are calculated using the feature vector of each exemplar. The predicted class of a testing image is the nearest class to that image in feature spaces. The algorithm is below:\n",
    "<img src=\"Figure_4.jpg\" width=\"400\">\n",
    "The arguments for using nearest-mean-of-exemplars instead of usual network outputs (softmax) are not clear in this paper. However, the ablation study shows the improved accuracy of using nearest-mean-of-exemplars.\n",
    "\n",
    "\n",
    "**Results.** \n",
    "1. Compare with previous methods on CIFAR100 and ILSVRC datasets. \n",
    "<img src=\"Figure_5.jpg\" width=\"800\">\n",
    "\n",
    "2. **Confusion matrices**\n",
    "\n",
    "3. **Ablation study:**\n",
    " hybrid1- use classification outputs.\n",
    " hybrid2- not use distillation loss.\n",
    " hybrid3- not use distillation loss + exemplars for classication, but use exemplars for represetnation learning.\n",
    " lwF.MC- a related method.\n",
    "<img src=\"Figure_6.jpg\" width=\"400\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
